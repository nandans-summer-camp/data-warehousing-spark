{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark Intro\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Map and Reduce\n",
    "\n",
    "We've talked about these concepts in the intro_python module, but we didn't talk explicity about reductions.\n",
    "\n",
    "MAP\n",
    "\n",
    "```python\n",
    "def squarer(a):\n",
    "    return a**2\n",
    "```\n",
    "\n",
    "REDUCE\n",
    "\n",
    "The prototypical reduce function is the sum function. A reducer takes two values: the first value is called an \"accumulator\" and the second value is the next value in the list. The goal of the reduce function is to transform the iterable into a new object that is \"accumulated\" as one iterates through the items in the iterable:\n",
    "\n",
    "```python\n",
    "def summer(a,b):\n",
    "    return a + b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "orders = pd.read_csv('data/orders.csv').to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# We will start with the basic reduce function in Python\n",
    "# NOTE: we have previously been using for comprehensions in python for mapping, \n",
    "# but there is also a built-in \"map\" function\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_total_sale(order):\n",
    "    return order['quantity_ordered'] * order['price_each']\n",
    "\n",
    "# What does this return???\n",
    "map(make_total_sale, orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def summer(a,b):\n",
    "    return a + b\n",
    "\n",
    "# Get the total sales!\n",
    "reduce(summer, map(make_total_sale, orders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Note, reduce takes an optional \"initial value\":\n",
    "\n",
    "# See what happens when you change it from 0!\n",
    "reduce(summer, map(make_total_sale, orders), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Challenge: make a \"map\" and a \"reduce\" that gets the max number of items ordered (in one line item)!\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Sometimes our data comes in JSON and it might be nested. Take a look at this format: \n",
    "\n",
    "import json\n",
    "\n",
    "with open('data/orders.json') as f:\n",
    "    orders = [json.loads(l) for l in f]\n",
    "\n",
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Challenge: Find the sum of the total number of sales with this new format.\n",
    "# Everything should be accomplished in a map and then a reduce:\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Spark lets us read JSON files and create dataframes with nested items!\n",
    "\n",
    "orders = spark.read.json('data/orders.json')\n",
    "\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# All \"DataFrames\" in Spark are backed by an RDD. \n",
    "# We can access the RDD to perform simple operations, just like in python,\n",
    "# on the underlying data:\n",
    "\n",
    "# Spark performs all operations lazily. \"collect\" tells it to gather all the items\n",
    "# in a list and send it back to the Driver node. \n",
    "# If you try and collect too much data, your memory will blow up!\n",
    "\n",
    "orders.rdd.filter(lambda r: r.customer.country == 'Belgium').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Challenge\n",
    "\n",
    "# Let's try and repeat the previous operation, of finding the total sales, \n",
    "# but now with the spark RDD. \n",
    "# You will need to read the Spark Documentation and find the following functions: map, reduce\n",
    "# which are available as methods directly on the RDD!\n",
    "# play around: \n",
    "\n",
    "\n",
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Spark thinks about where the Data lives. \n",
    "# RDD's have a concept of \"key, value\"\n",
    "# This is implemented simply as a Tuple: (k,v)\n",
    "\n",
    "# If we create an RDD that takes that form, a tuple of (k,v)\n",
    "# we can use operations like \"reduceByKey\"!\n",
    "\n",
    "\n",
    "# Challenge\n",
    "\n",
    "# Get the total sales by country, by first mapping your RDD into a tuple (k,v) where\n",
    "# the Key is the country, then reducing by summing the total sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Parquet\n",
    "\n",
    "Parquet is another data format that plays well with Spark. \n",
    " \n",
    "It's a \"flat file\" format, like JSON or CSV, but it contains extra information about types, allows for \"predicate pushdown\", is column-oriented, and has first-class support for nested columns!\n",
    "\n",
    "Predicate pushdown means that Spark doesn't have to read all the data from the disk! It can avoid certain sections of disk altogether because Parquet knows that we don't want that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.read.parquet('data/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Here, Spark won't read any information about countries other than Belgium!\n",
    "# NOTE: the nested type!\n",
    "\n",
    "orders.createOrReplaceTempView('orders')\n",
    "\n",
    "res = spark.sql(\"\"\"\n",
    "SELECT count(order_number)\n",
    "FROM orders \n",
    "WHERE customer.country = 'Belgium'\n",
    "\"\"\".strip())\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Nested types in SQL!\n",
    "\n",
    "How do we deal with these pesky nested types now? \n",
    "\n",
    "Spark SQL gives us the Map/Reduce formulas to deal with nested \"Array\" types!\n",
    "\n",
    "map = TRANSFORM\n",
    "reduce = AGGREGATE\n",
    "\n",
    "You can look at the documentation to see exactly how they work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Bonus Challenge: \n",
    "\n",
    "# Try to reproduce what we did before, getting the total sales, in Spark SQL, using TRANSFORM and AGGREGATE on the individuals \"line_items\" and then summing over the rows to get the total sales"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/opt/conda/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "mapreduce.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
