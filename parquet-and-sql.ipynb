{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Parquet\n",
    "\n",
    "Parquet is another data format that plays well with Spark. \n",
    " \n",
    "It's a \"flat file\" format, like JSON or CSV, but it contains extra information about types, allows for \"predicate pushdown\", is column-oriented, and has first-class support for nested columns!\n",
    "\n",
    "Predicate pushdown means that Spark doesn't have to read all the data from the disk! It can avoid certain sections of disk altogether because Parquet knows that we don't want that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.read.parquet('data/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Here, Spark won't read any information about countries other than Belgium!\n",
    "# NOTE: the nested type!\n",
    "\n",
    "orders.createOrReplaceTempView('orders')\n",
    "\n",
    "res = spark.sql(\"\"\"\n",
    "SELECT count(order_number)\n",
    "FROM orders \n",
    "WHERE customer.country = 'Belgium'\n",
    "\"\"\".strip())\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Nested types in SQL!\n",
    "\n",
    "How do we deal with these pesky nested types now? \n",
    "\n",
    "Spark SQL gives us built-in functions to deal with nested \"Array\" types!\n",
    "\n",
    "1. TRANSFORM: this is a `map` operation.\n",
    "2. AGGREGATE: a slightly more general form of `reduce`.\n",
    "\n",
    "You can look at the documentation to see exactly how they work: \n",
    "\n",
    "https://spark.apache.org/docs/latest/api/sql/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1:\n",
    "\n",
    "# Try to reproduce what we did before, getting the total sales, in Spark SQL, \n",
    "# using TRANSFORM and AGGREGATE on the individuals \"line_items\" and then \n",
    "# summing over the rows to get the total sales.\n",
    "\n",
    "# HINT: Using aggregate you need to get the types right, which can be a bit \n",
    "# confusing (bigint aka long, vs int aka short)\n",
    "\n",
    "# HINT2: You can alternatively use EXPLODE. Note this works similar to flatMap \n",
    "# in rdd language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/opt/conda/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "name": "parquet-and-sql.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
