{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark \n",
    "\n",
    "Spark is a big-data processing framework that I describe as follows: \n",
    "\n",
    "1. We have a lot of data.\n",
    "2. We will split our data into chunks, called \"partitions\".\n",
    "3. We read data in partitions, we write data in partitions, and all the processing in between, happens in partitions. \n",
    "4. Given that data is in partitions, each partition can be worked on in parallel, in serial, or a combination of both. \n",
    "5. Much of the \"behind-the-scenes\" magic of Spark consists of scheduling the work of processing each partition to a different \"worker\", where workers are potentially spread across machines.\n",
    "6. Much of the day-to-day work of a Spark programmer consists of trying to compute what you want to compute, given the constraint that your data is all split up into a bunch of f(*&#@ing partitions.\n",
    "\n",
    "Spark is succesful because this framework of splitting data up into partitions is a very useful abstraction that's extremely _flexible_. You can work with very very large data, do basically anything you want with the data, and do so across any number of machines. That's very powerful! \n",
    " \n",
    "We already know one basic pattern that is super efficient in this framework: map + reduce! By it's very definition, map works on each partition (indeed, one each \"row\" or element individually) and doesn't need to perform any operation that spans partitions. On the other hand, Reduce is an aggregation operation that successively \"folds\" each element into an \"accumulator\", and can thus be used to combine data across partitions into a final result. As long as our reduce operation is both associative and commutative, like the _sum_ operation, this works easily with partitioned data: we just sum within partitions, then sum the results across partitions! \n",
    "\n",
    "Let's practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# We start by creating what is called a \"SparkSession\":\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark Intro\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Spark lets us read many file types, including JSON. \n",
    "# Reading, like most operations in Spark, is lazy,\n",
    "# so tihs operation won't read the data into memory,\n",
    "# it will just scan the data to learn the \"schema\".\n",
    "orders = spark.read.json('data/orders.json')\n",
    "\n",
    "# orders is an instance of a Spark DataFrame. \n",
    "# .printSchema is a method that can be used to\n",
    "# see that \"schema\" of the DataFrame. Let's take\n",
    "# a look at what we mean: \n",
    "\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames + RDDs\n",
    "\n",
    "The RDD (Resilient Distributed Dataset) is the fundamental data structure of Spark. You can think of it as a partitioned list!\n",
    "\n",
    "DataFrame's are a higher-level construct built on top of RDDs. They have \"schemas\" which means that each column has a \"type\" (ie string, int, etc.). Unlike our basic, old-school SQL database, columns can be nested! Because the columns have types, Spark can optimize operations in a Dataframe. In general, they allow us to easily write SQL-style operations on our data with super-optimized execution under-the-hood. This is great for a large set of analytics use-cases!\n",
    "\n",
    "The basic rule of thumb is: use DataFrames if you can fit what you're trying to do within their API, because it will usually be faster, otherwise use the RDDs directly if you need the flexibility. \n",
    "\n",
    "For this tutorial, we will be focusing on RDDs, because it's educational and simple to get started with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access the RDD of a DataFrame to perform simple operations on the\n",
    "# data with basic Python functions.\n",
    "\n",
    "# Let's take a look at the RDD:\n",
    "\n",
    "orders.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember: all operations in Spark are lazy. So we haven't actually\n",
    "# read the data from the file yet.\n",
    "\n",
    "# We can take a look at the first few elements in the data with the .take\n",
    "# method: \n",
    "\n",
    "orders.rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA\n",
      "Germany\n"
     ]
    }
   ],
   "source": [
    "# Notice that .take returns a list of elements. As we said, \n",
    "# an RDD is like a distributed, lazy, partitioned list. When we\n",
    "# run .take, we bring all the elements into memory into a list.\n",
    "\n",
    "# What are the elements? Well this row was created from a DataFrame,\n",
    "# so all of the elements are instance of the Spark Row class!\n",
    "\n",
    "# Row's in Spark are a lot like Row's in pandas. We can access the \n",
    "# individual values with \"dot notation\":\n",
    "\n",
    "for r in orders.rdd.take(2):\n",
    "    print(r.customer.country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .collect is like .take, but it brings ALL the data into a list\n",
    "# If you try and collect too much data, your memory will blow up!\n",
    "\n",
    "# Note some things here:\n",
    "\n",
    "# 1. RDDs have a method called .filter\n",
    "# 2. RDDs let us use basic Python functions to do all the work!\n",
    "# 3. We can read about the RDD methods here: \n",
    "#    https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD\n",
    "\n",
    "orders.rdd.filter(lambda r: r.customer.country == 'Belgium').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "\n",
    "# Let's try and repeat the operation from the Python map-reduce tutorial, \n",
    "# of finding the total sales, but now with the spark RDD. \n",
    "# You will need to read the Spark Documentation and find the following functions: map, reduce\n",
    "# which are available as methods directly on the RDD!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "\n",
    "# Take a look at the RDD method .flatMap\n",
    "# Now try and use that method to rewrite your previous function!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 3\n",
    "\n",
    "# RDD's have a concept of \"key, value\" which\n",
    "# Is implemented simply as a Tuple: (k,v). \n",
    "# So if we make an RDD of tuples, we have some\n",
    "# special methods that will operate on them, \n",
    "# such as .mapValues or .reduceByKey\n",
    "\n",
    "# Get the total sales by country, by first mapping \n",
    "# your RDD into a tuple (k,v) where the Key is the \n",
    "# country, then reducing by summing the total sales. \n",
    "# You can use .collectAsMap to turn the list of \n",
    "# tuples into a Dictionary too!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "name": "spark-tutorial.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
